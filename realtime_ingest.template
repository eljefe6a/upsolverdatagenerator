/*. 
This template guides you through building a pipeline that reads data from two Kafka topics, join the data from the Kafka topics, and writes transformed results to a Snowflake table.
The steps include:
    1.  Connect to your Kafka cluster. This template uses a public bootstrap server.
    2.  Create empty staging tables to host the raw web traffic and user data.
    3.  Create jobs to ingest the raw data from the Kafka topic to the staging tables.
    4.  Create a Materialized View to aggregate the user data
    5.  Create an output table in AWS Glue Data Catalog.
    6.  Create a job to join the web traffic data with the user data in our Materialized View.
    7.  Create a connection to the Snowflake database.
    8.  Create an empty table in Snowflake to host the joined and cleansed (transformed) data.    
    9.  Create a job to transform, join, and stream the results to your output Snowflake table.
    10. Query the data in Snowflake.
    11. Clean up the connections, views, and data.
*/


/*
    1. Connect to your Kafka cluster using a public bootstrap server. 
    If you don't have an easily accessible Kafka cluster, you can sign up for a free Confluent cloud account for testing. 

    Example code:

    CREATE KAFKA CONNECTION my_kafka_connection
       HOSTS = ('pkc-2396y.us-east-1.aws.confluent.cloud:9092')
       CONSUMER_PROPERTIES = '
         bootstrap.servers=pkc-2396y.us-east-1.aws.confluent.cloud:9092
         security.protocol=SASL_SSL
         sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule   required username="XXXXXXXX"   password="-----------";
         ssl.endpoint.identification.algorithm=https
         sasl.mechanism=PLAIN
       ';
*/ 
CREATE KAFKA CONNECTION KAFKA_CONNECTION
      HOSTS = ('pkc-2396y.us-east-1.aws.confluent.cloud:9092') 
      CONSUMER_PROPERTIES = 'pkc-2396y.us-east-1.aws.confluent.cloud:9092
      security.protocol=SASL_SSL
      sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule   required username="MQZLJDZ2YWGT2R7T"   password="4UdYTH3Ok1fznHcLLAeM4QPUw/1QY1ZxMaDqUmW9dpQJKEmzVwJQtnAedWBWXq1R";
      ssl.endpoint.identification.algorithm=https
      sasl.mechanism=PLAIN';

/*
    2. Create staging tables to store your raw data from Kafka. 
*/
CREATE TABLE default_glue_catalog.database_fc91d1.rt_web_traffic_data(
            host string,
            user_id string,
            time_stamp string,
            http_method string,
            url string,
            protocol string,
            status_code string,
            browser_client string
        )
        PARTITIONED BY $event_date
        PRIMARY KEY user_id;

    CREATE TABLE default_glue_catalog.database_fc91d1.rt_user_data()
        PARTITIONED BY $event_date;
        

/*    
    3. Create a job to ingest raw streaming data from your Kafka topic to the staging table.
    
    Note: It may take a 3-4 minutes for the data to appear in your output table.
*/
CREATE JOB rt_web_traffic_data_job
    START_FROM = BEGINNING
    CONTENT_TYPE = JSON
    AS COPY FROM KAFKA KAFKA_CONNECTION TOPIC = 'apache_log_json' 
    INTO default_glue_catalog.database_fc91d1.rt_web_traffic_data; 

CREATE JOB rt_user_info_job
    START_FROM = BEGINNING
    CONTENT_TYPE = JSON
    ALLOW_EMPTY_SOURCES = true
    AS COPY FROM KAFKA KAFKA_CONNECTION TOPIC = 'user_info_json' 
    INTO default_glue_catalog.database_fc91d1.rt_user_data; 

/* Query the newly populated data in the staging table, created in step 2. */
SELECT * FROM default_glue_catalog.database_fc91d1.rt_web_traffic_data limit 10;

SELECT * FROM default_glue_catalog.database_fc91d1.rt_user_data limit 10;

/*
    4. Create a Materialized View to aggregate the user data
    
    A Materialized View holds key-value pairs where the key is used to join with another table.
    Note: You need to have data in your user data raw data table before creating the materialized view.
*/
    CREATE MATERIALIZED VIEW default_glue_catalog.database_fc91d1.rt_physical_user_data_materialized_view AS 
        SELECT user_id,
           LAST(LAST_ELEMENT(SPLIT(address, ' '))) as zip_code,
           LAST(MD5(credit_card)) as credit_card,
           LAST(first_name) as first_name,
           LAST(last_name) as last_name,
           LAST(MD5(password)) as password,
           LAST(phone) as phone,
           LAST(subscription) as subscription
        FROM default_glue_catalog.database_fc91d1.rt_user_data
        GROUP BY user_id;

/*
    5. Create an output table in AWS Glue Data Catalog. The table will be availalble in Athena once data has been added.
*/
    CREATE TABLE default_glue_catalog.database_fc91d1.rt_joined_web_traffic_data(
           host STRING,
           user_id STRING,
           zip_code STRING,
           credit_card STRING,
           first_name STRING,
           last_name STRING,
           password STRING,
           phone STRING,
           subscription STRING,
           url STRING
           )
    PRIMARY KEY user_id;


/*
    6. Join the web traffic data with the user data in our Materialized View

    Note: You need to have data in your materialized view before creating the job.
*/
    CREATE JOB rt_join_user_data_and_web_traffic
        START_FROM = BEGINNING
        ADD_MISSING_COLUMNS = TRUE
        RUN_INTERVAL = 1 MINUTE
        AS INSERT INTO default_glue_catalog.database_fc91d1.rt_joined_web_traffic_data map_columns_by_name
        SELECT 
           s.user_id as user_id,
           m_v.credit_card as credit_card,
           m_v.first_name as first_name,
           m_v.last_name as last_name,
           m_v.password as password,
           m_v.phone as phone,
           m_v.subscription as subscription,
           s.host as host,
           s.url as url
        FROM default_glue_catalog.database_fc91d1.rt_web_traffic_data as s
        LEFT JOIN default_glue_catalog.database_fc91d1.rt_physical_user_data_materialized_view AS m_v
        ON s.user_id = m_v.user_id
        WHERE $commit_time between run_start_time() AND run_end_time();
        
    /** Your pipeline is now running, bringing data from the source Kafka topics into staging tables, transforming it, and saving it to an output table. **/

    /* 
       Query the output table to view the results of the transformation job. SQLake queries the table using the Athena APIs.  
       Note: It may take 3-4 minutes for the data to appear in your output table.
    */
    SELECT * FROM default_glue_catalog.database_fc91d1.rt_joined_web_traffic_data LIMIT 10;

/*
    7. Create a Snowflake JDBC connection with the proper permissions.
    
    Example code:
    
    CREATE SNOWFLAKE CONNECTION SFCommerce
       CONNECTION_STRING = 'jdbc:snowflake://baa12345.us-east-1.snowflakecomputing.com/?db=DEMO_DB'
       USER_NAME = 'demouser'
       PASSWORD = 'demopass';
*/
    CREATE SNOWFLAKE CONNECTION snowflake_connection
        CONNECTION_STRING = 'jdbc:snowflake://wub03701.us-east-1.snowflakecomputing.com/?db=DEMO&warehouse=COMPUTE_WH'
        USER_NAME = 'jesse'
        PASSWORD = 'bIxyf71xHVRvFsHaYu';


/*     
    8. Go to Snowflake and create a new table. This is the target table that you want to populate with transformations applied. 
    
    CREATE or REPLACE TABLE rt_joined_web_traffic_data (
           user_id VARCHAR(100),
           zip_code VARCHAR(20),
           credit_card VARCHAR(32),
           first_name VARCHAR(50),
           last_name VARCHAR(50),
           password VARCHAR(32),
           phone VARCHAR(20),
           subscription VARCHAR(20),
           host VARCHAR(100),
           url VARCHAR(200)
    );
*/

/*     
    9. Create a job to streams transformed data to your snowflake table created in the previous step
    
    Note: You need to have data in your staging table before creating the job.
*/
CREATE JOB "snowflake_loader"
       RUN_INTERVAL = 1 MINUTE
       START_FROM = BEGINNING
       AS INSERT INTO SNOWFLAKE snowflake_connection.PUBLIC.RT_JOINED_WEB_TRAFFIC_DATA MAP_COLUMNS_BY_NAME
               /* map the staging columns to your snowflake table columns created from the previous step */
          SELECT USER_ID as USER_ID,
                ZIP_CODE as ZIP_CODE,
                CREDIT_CARD as CREDIT_CARD,
                FIRST_NAME as FIRST_NAME,
                LAST_NAME as LAST_NAME,
                PASSWORD as PASSWORD,
                PHONE as PHONE,
                SUBSCRIPTION as SUBSCRIPTION,
                HOST as HOST,
                URL as URL
          FROM default_glue_catalog.database_fc91d1.rt_joined_web_traffic_data
          WHERE ($commit_time between run_start_time() AND run_end_time());
 
/** Your pipeline is now running, bringing data from the source Kafka into a staging table, transforming it and saving it to an output table in Snowflake. **/
 
/*
    10. You can now go to Snowflake and query the table created in Step #5.
    
    SELECT * FROM DEMO.PUBLIC.RT_JOINED_WEB_TRAFFIC_DATA LIMIT 10;
*/

/*
    11. Clean up all jobs, tables, and views.
*/
DROP JOB rt_join_user_data_and_web_traffic;

DROP TABLE default_glue_catalog.database_fc91d1.rt_jh_joined_web_traffic_data
DELETE_DATA = TRUE;

DROP MATERIALIZED VIEW default_glue_catalog.database_fc91d1.rt_physical_user_data_materialized_view;

DROP JOB rt_web_traffic_data_job; 
DROP JOB rt_user_info_job;

DROP CONNECTION KAFKA_CONNECTION;

DROP TABLE default_glue_catalog.database_fc91d1.rt_web_traffic_data
DELETE_DATA = TRUE;
DROP TABLE default_glue_catalog.database_fc91d1.rt_user_data
DELETE_DATA = TRUE;
