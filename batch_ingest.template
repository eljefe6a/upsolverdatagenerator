    /*
    This template walks through how to join multiple S3 data sources and write the results to Snowflake
    using generated Apache log web traffic and user data.

    The steps include:
        1.  Create a connection to S3
        2.  Create two staging tables to ingest the web traffic and user data.
        3.  Create two jobs to load the source data into the staging tables.
        4.  Create materialized views for for the web traffic and user data tables.
        5.  Create an output table in AWS Glue Data Catalog.
        6.  Create a job to join the web traffic data with the user data in our Materialized View.
        7.  Create a connection to the Snowflake database.
        8.  Create an empty table in Snowflake to host the cleansed (transformed) data.
        9.  Create a job to start transforming, joining, and streaming the cleansed data to your Snowflake table.
        10. Query the data in Snowflake.
        11. Clean up the connections, views, and data.
    */

    /* 
        1. Create an S3 connection 
           
        This connection provides the necessary IAM credentials for jobs that need to access data in S3.
        We will use this connection to read both the user data and the web traffic data.

        Follow these instructions to get S3 connected to Upsolver
        https://docs.upsolver.com/sqlake/connectors/connect-to-your-s3-bucket/configure-access-to-s3.
    */
    CREATE S3 CONNECTION upsolver_s3_sample_data
        AWS_ROLE = 'arn:aws:iam::484601517394:role/upsolvers3role'
        EXTERNAL_ID = 'GTWKRE6K'
        READ_ONLY = TRUE;

    /*
        2. Create the staging tables for web traffic and user data.
    */
    CREATE TABLE default_glue_catalog.database_fc91d1.web_traffic_data(
            host string,
            user_id string,
            time_stamp string,
            http_method string,
            url string,
            protocol string,
            status_code string,
            browser_client string
        )
        PARTITIONED BY $event_date
        PRIMARY KEY user_id;

    CREATE TABLE default_glue_catalog.database_fc91d1.user_data()
        PARTITIONED BY $event_date;

    /*
        3. Create two jobs to ingest raw web traffic and user data into the staging tables.
        
        Note: It may take a 3-4 minutes for the data to appear in your output table.
    */

    /* Job to Apache Log data. */
    CREATE JOB load_web_traffic_data_from_s3
       CONTENT_TYPE = AVRO
       AS COPY FROM S3 upsolver_s3_sample_data 
          BUCKET = 'myupsolverdemo' 
          PREFIX = 'apache_log_data/' 
       INTO default_glue_catalog.database_fc91d1.web_traffic_data; 

    /* Job to ingest user info data. */
    CREATE JOB load_user_data_from_s3
       CONTENT_TYPE = AVRO
       AS COPY FROM S3 upsolver_s3_sample_data 
          BUCKET = 'myupsolverdemo' 
          PREFIX = 'user_info_data/'
       INTO default_glue_catalog.database_fc91d1.user_data;

    /*
        Query the raw data from the staging tables
        
        Note: It may take 3-4 minutes for the data to appear in your output table.
    */
    SELECT * FROM default_glue_catalog.database_fc91d1.web_traffic_data LIMIT 10;
    SELECT * FROM default_glue_catalog.database_fc91d1.user_data LIMIT 10;

    /*
        4. Create a Materialized View to aggregate the user data
        
        A Materialized View holds key-value pairs where the key is used to join with another table.
        Note: You need to have data in your user data raw data table before creating the materialized view.
    */
    CREATE MATERIALIZED VIEW default_glue_catalog.database_fc91d1.physical_user_data_materialized_view AS 
        SELECT user_id,
           LAST(LAST_ELEMENT(SPLIT(address, ' '))) as zip_code,
           LAST(MD5(credit_card)) as credit_card,
           LAST(first_name) as first_name,
           LAST(last_name) as last_name,
           LAST(MD5(password)) as password,
           LAST(phone) as phone,
           LAST(subscription) as subscription
        FROM default_glue_catalog.database_fc91d1.user_data
        GROUP BY user_id;

    /*
        5. Create an output table in AWS Glue Data Catalog. The table will be availalble in Athena once data has been added.
    */
    CREATE TABLE default_glue_catalog.database_fc91d1.joined_web_traffic_data(
           host STRING,
           user_id STRING,
           zip_code STRING,
           credit_card STRING,
           first_name STRING,
           last_name STRING,
           password STRING,
           phone STRING,
           subscription STRING,
           url STRING
           )
    PRIMARY KEY user_id;

    /*
        6. Join the web traffic data with the user data in our Materialized View

        Note: You need to have data in your materialized view before creating the job.
    */
    CREATE JOB join_user_data_and_web_traffic
        START_FROM = BEGINNING
        ADD_MISSING_COLUMNS = TRUE
        RUN_INTERVAL = 1 MINUTE
        AS INSERT INTO default_glue_catalog.database_fc91d1.joined_web_traffic_data map_columns_by_name
        SELECT 
           m_v.user_id as user_id,
           m_v.zip_code as zip_code,
           m_v.credit_card as credit_card,
           m_v.first_name as first_name,
           m_v.last_name as last_name,
           m_v.password as password,
           m_v.phone as phone,
           m_v.subscription as subscription,
           s.host as host,
           s.url as url
        FROM default_glue_catalog.database_fc91d1.web_traffic_data as s
        LEFT JOIN default_glue_catalog.database_fc91d1.physical_user_data_materialized_view AS m_v
        ON s.user_id = m_v.user_id
        WHERE $commit_time between run_start_time() AND run_end_time();
        
    /** Your pipeline is now running, bringing data from the source S3 bucket into a staging table, transforming it and saving it to an output table. **/

    /* 
       Query the output table to view the results of the transformation job. SQLake queries the table using the Athena APIs.  
       Note: It may take 3-4 minutes for the data to appear in your output table.
    */
    SELECT * FROM default_glue_catalog.database_fc91d1.joined_web_traffic_data LIMIT 10;

    /*     7. Create a Snowflake jdbc connection. Make sure you have proper permissions for your database. 
    */

    CREATE SNOWFLAKE CONNECTION snowflake_connection
        CONNECTION_STRING = 'changeme'
        USER_NAME = 'changeme'
        PASSWORD = 'changeme';

    /*     8. Go to Snowflake and create a new table. This is the target table that you want to populate with transformations applied. 
           Example DDL for a Snowflake table is provided here. You need to run this in your Snowflake environment:
           
           Sample code:
           CREATE or REPLACE TABLE joined_web_traffic_data (
                user_id VARCHAR(100),
                zip_code VARCHAR(20),
                credit_card VARCHAR(32),
                first_name VARCHAR(50),
                last_name VARCHAR(50),
                password VARCHAR(32),
                phone VARCHAR(20),
                subscription VARCHAR(20),
                host VARCHAR(100),
                url VARCHAR(200)
           );
    */

    /*     9. Create a job to start streaming transformed data to your snowflake table created previously   
            Note: You need to have data in your staging table before creating the job.  
    */

    CREATE JOB "snowflake_loader"
       RUN_INTERVAL = 1 MINUTE
       START_FROM = BEGINNING
       AS INSERT INTO SNOWFLAKE "snowflake_connection"."PUBLIC"."JOINED_WEB_TRAFFIC_DATA" MAP_COLUMNS_BY_NAME
               /* map the staging columns to your snowflake table columns created from the previous step */
          SELECT USER_ID as USER_ID,
                ZIP_CODE as ZIP_CODE,
                CREDIT_CARD as CREDIT_CARD,
                FIRST_NAME as FIRST_NAME,
                LAST_NAME as LAST_NAME,
                PASSWORD as PASSWORD,
                PHONE as PHONE,
                SUBSCRIPTION as SUBSCRIPTION,
                HOST as HOST,
                URL as URL
          FROM default_glue_catalog.database_fc91d1.joined_web_traffic_data
          WHERE ($commit_time between run_start_time() AND run_end_time());
     
     /*     10. You can now go to Snowflake and query the table you created in step #5 to make sure the data is being streamed properly. 
     
            Example code: 
            SELECT * FROM DEMO.PUBLIC.JOINED_WEB_TRAFFIC_DATA LIMIT 10;
     */

     /*     11. Clean up.
     */
     DROP JOB snowflake_loader;
     DROP CONNECTION snowflake_connection;
     DROP JOB join_user_data_and_web_traffic;

     DROP TABLE default_glue_catalog.database_fc91d1.joined_web_traffic_data DELETE_DATA = TRUE;
     DROP MATERIALIZED VIEW default_glue_catalog.database_fc91d1.physical_user_data_materialized_view;

     DROP JOB load_web_traffic_data_from_s3;
     DROP JOB load_user_data_from_s3;

     DROP TABLE default_glue_catalog.database_fc91d1.web_traffic_data DELETE_DATA = FALSE;
     DROP TABLE default_glue_catalog.database_fc91d1.user_data DELETE_DATA = FALSE;
     